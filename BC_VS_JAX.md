# PyTorch BC vs JAX ç‰ˆæœ¬å¯¹æ¯”

## ä»£ç ç»“æ„å¯¹åº”å…³ç³»

### æ–‡ä»¶ç»„ç»‡

| JAX ç‰ˆæœ¬ | PyTorch ç‰ˆæœ¬ | è¯´æ˜ |
|---------|-------------|------|
| `main.py` | `train_bc.py` | ä¸»è®­ç»ƒè„šæœ¬ |
| `agents/acfql_agent.py` | `agents/bc_agent.py` | Agent å®ç° |
| `utils/flax_utils.py` | - | PyTorch ä¸éœ€è¦ï¼ˆç›´æ¥ç”¨ torchï¼‰ |
| `utils/datasets.py` | `utils/datasets.py` | æ•°æ®é›†ï¼ˆå·²é€‚é… PyTorchï¼‰ |
| `evaluation.py` | `evaluation.py` | è¯„ä¼°å‡½æ•°ï¼ˆå·²é€‚é…ï¼‰ |

### ä¸»è¦å‡½æ•°å¯¹åº”

| JAX ç‰ˆæœ¬ | PyTorch ç‰ˆæœ¬ |
|---------|-------------|
| `agent.create()` | `BCAgent.create()` |
| `agent.update(batch)` | `agent.update(batch)` |
| `agent.batch_update(batch)` | `agent.batch_update(batch)` |
| `agent.sample_actions(obs, rng=key)` | `agent.sample_actions(obs)` |
| `jax.random.split()` | PyTorch å†…ç½®éšæœºæ•°ç®¡ç† |

---

## è®­ç»ƒè„šæœ¬å¯¹æ¯”

### main() å‡½æ•°ç»“æ„

ä¸¤ä¸ªç‰ˆæœ¬çš„ `main()` å‡½æ•°ç°åœ¨å…·æœ‰**ç›¸åŒçš„ç»“æ„**ï¼š

```python
# ===== 1. Setup =====
exp_name = get_exp_name(FLAGS.seed)
run = setup_wandb(...)
FLAGS.save_dir = os.path.join(...)
# ä¿å­˜ flags.json

# ===== 2. House keeping =====
random.seed(FLAGS.seed)
np.random.seed(FLAGS.seed)
# PyTorch: torch.manual_seed(FLAGS.seed)
# JAX: jax.random.PRNGKey(FLAGS.seed)

log_step = 0
discount = FLAGS.discount

# ===== 3. Data loading =====
env, eval_env, train_dataset, val_dataset = make_env_and_datasets(FLAGS.env_name)

# ===== 4. Handle dataset =====
def process_train_dataset(ds):
    # å¤„ç† dataset proportion
    # å¤„ç† sparse reward
    # å¤„ç† robomimic reward
    return ds

train_dataset = process_train_dataset(train_dataset)
example_batch = train_dataset.sample(FLAGS.batch_size)

# ===== 5. Create agent =====
# JAX: agent = agent_class.create(FLAGS.seed, obs, actions, config)
# PyTorch: agent = BCAgent.create(observation_shape, action_dim, config)

# ===== 6. Setup logging =====
prefixes = ["eval", "offline_agent"]
logger = LoggingHelper(csv_loggers={...}, wandb_logger=wandb)

# ===== 7. Offline training loop =====
for i in tqdm.tqdm(range(1, FLAGS.offline_steps + 1)):
    log_step += 1
    
    # Sample batch
    batch = train_dataset.sample_sequence(...) or train_dataset.sample(...)
    
    # Update agent
    # JAX: agent, offline_info = agent.update(batch)
    # PyTorch: offline_info = agent.update(batch)
    
    if i % FLAGS.log_interval == 0:
        logger.log(offline_info, "offline_agent", step=log_step)
    
    # Saving
    if FLAGS.save_interval > 0 and i % FLAGS.save_interval == 0:
        # JAX: save_agent(agent, FLAGS.save_dir, log_step)
        # PyTorch: agent.save(checkpoint_path)
    
    # Eval
    if i == FLAGS.offline_steps - 1 or (FLAGS.eval_interval != 0 and i % FLAGS.eval_interval == 0):
        eval_info, _, renders = evaluate(...)
        logger.log(eval_info, "eval", step=log_step)

# ===== 8. Cleanup =====
for key, csv_logger in logger.csv_loggers.items():
    csv_logger.close()

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
with open(os.path.join(FLAGS.save_dir, 'token.tk'), 'w') as f:
    f.write(run.url)
```

---

## å‘½ä»¤è¡Œå‚æ•°å¯¹æ¯”

### å®Œå…¨ç›¸åŒçš„å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--run_group` | 'BC' / 'Debug' | è¿è¡Œç»„ |
| `--seed` | 0 | éšæœºç§å­ |
| `--env_name` | ... | ç¯å¢ƒåç§° |
| `--save_dir` | 'runs/fbc/' / 'exp/' | ä¿å­˜ç›®å½• |
| `--offline_steps` | 1000000 | ç¦»çº¿è®­ç»ƒæ­¥æ•° |
| `--log_interval` | 5000 | æ—¥å¿—é—´éš” |
| `--eval_interval` | 100000 | è¯„ä¼°é—´éš” |
| `--save_interval` | -1 | ä¿å­˜é—´éš” |
| `--discount` | 0.99 | æŠ˜æ‰£å› å­ |
| `--eval_episodes` | 50 | è¯„ä¼° episode æ•° |
| `--video_episodes` | 0 | è§†é¢‘ episode æ•° |
| `--video_frame_skip` | 3 | è§†é¢‘å¸§è·³è¿‡ |
| `--dataset_proportion` | 1.0 | æ•°æ®é›†æ¯”ä¾‹ |
| `--horizon_length` | 5 | Action chunking é•¿åº¦ |
| `--sparse` | False | ç¨€ç–å¥–åŠ± |

### PyTorch ç‰ˆæœ¬ç§»é™¤çš„å‚æ•°

| JAX å‚æ•° | ç§»é™¤åŸå›  |
|---------|---------|
| `--ogbench_dataset_dir` | ä¸éœ€è¦æœ¬åœ° OGBench æ•°æ® |
| `--dataset_replace_interval` | ç®€åŒ–è®­ç»ƒæµç¨‹ |
| `--online_steps` | åªåš BCï¼Œä¸åš online RL |
| `--buffer_size` | BC ä¸éœ€è¦ replay buffer |
| `--start_training` | BC ä¸éœ€è¦é¢„å¡«å…… buffer |
| `--utd_ratio` | BC ä¸éœ€è¦ update-to-data ratio |

### PyTorch ç‰ˆæœ¬æ–°å¢çš„å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--lr` | 3e-4 | å­¦ä¹ ç‡ï¼ˆJAX åœ¨ config æ–‡ä»¶ä¸­ï¼‰ |
| `--batch_size` | 256 | Batch sizeï¼ˆJAX åœ¨ config æ–‡ä»¶ä¸­ï¼‰ |
| `--flow_steps` | 10 | Flow æ­¥æ•°ï¼ˆJAX åœ¨ config æ–‡ä»¶ä¸­ï¼‰ |
| `--action_chunking` | False | æ˜¯å¦å¯ç”¨ action chunking |
| `--use_fourier_features` | False | æ˜¯å¦ä½¿ç”¨ Fourier features |
| `--encoder` | None | è§†è§‰ç¼–ç å™¨ |
| `--weight_decay` | 0.0 | L2 æ­£åˆ™åŒ– |

---

## Agent API å¯¹æ¯”

### åˆ›å»º Agent

```python
# JAX ç‰ˆæœ¬
agent = ACFQLAgent.create(
    seed=FLAGS.seed,
    ex_observations=example_batch['observations'],
    ex_actions=example_batch['actions'],
    config=config,
)

# PyTorch ç‰ˆæœ¬
agent = BCAgent.create(
    observation_shape=obs_shape,
    action_dim=action_dim,
    config=config,
)
```

### æ›´æ–° Agent

```python
# JAX ç‰ˆæœ¬ï¼ˆå‡½æ•°å¼ï¼Œè¿”å›æ–° agentï¼‰
agent, info = agent.update(batch)

# PyTorch ç‰ˆæœ¬ï¼ˆåŸåœ°æ›´æ–°ï¼Œè¿”å› infoï¼‰
info = agent.update(batch)
```

### é‡‡æ ·åŠ¨ä½œ

```python
# JAX ç‰ˆæœ¬ï¼ˆéœ€è¦æ˜¾å¼ä¼ å…¥ RNGï¼‰
rng, key = jax.random.split(rng)
actions = agent.sample_actions(observations=obs, rng=key)

# PyTorch ç‰ˆæœ¬ï¼ˆä½¿ç”¨å†…ç½®éšæœºæ•°ï¼‰
actions = agent.sample_actions(observations=obs)
```

### ä¿å­˜/åŠ è½½

```python
# JAX ç‰ˆæœ¬
from utils.flax_utils import save_agent, load_agent
save_agent(agent, save_dir, step)
agent = load_agent(path, agent)

# PyTorch ç‰ˆæœ¬
agent.save('checkpoint.pt')
agent.load('checkpoint.pt')
```

---

## æ•°æ®å¤„ç†å¯¹æ¯”

### é‡‡æ · Batch

```python
# ä¸¤ä¸ªç‰ˆæœ¬å®Œå…¨ç›¸åŒ
if FLAGS.action_chunking:
    batch = train_dataset.sample_sequence(
        batch_size=FLAGS.batch_size,
        sequence_length=FLAGS.horizon_length,
        discount=discount
    )
else:
    batch = train_dataset.sample(FLAGS.batch_size)
```

### è½¬æ¢ä¸º Tensor

```python
# JAX ç‰ˆæœ¬
# batch å·²ç»æ˜¯ JAX arraysï¼Œä¸éœ€è¦è½¬æ¢

# PyTorch ç‰ˆæœ¬
batch_tensor = {
    'observations': torch.from_numpy(batch['observations']).float(),
    'actions': torch.from_numpy(batch['actions']).float(),
    # ...
}
```

---

## è¯„ä¼°å¯¹æ¯”

### è¯„ä¼°å‡½æ•°è°ƒç”¨

```python
# ä¸¤ä¸ªç‰ˆæœ¬å®Œå…¨ç›¸åŒ
eval_info, trajs, renders = evaluate(
    agent=agent,
    env=eval_env,
    action_dim=action_dim,
    num_eval_episodes=FLAGS.eval_episodes,
    num_video_episodes=FLAGS.video_episodes,
    video_frame_skip=FLAGS.video_frame_skip,
)

if len(renders) > 0:
    eval_info['video'] = get_wandb_video(
        renders, 
        fps=int(30 / max(FLAGS.video_frame_skip, 1))
    )

logger.log(eval_info, "eval", step=log_step)
```

### åŠ¨ä½œæ‰§è¡Œï¼ˆevaluation.py ä¸­ï¼‰

```python
# ä¸¤ä¸ªç‰ˆæœ¬ç›¸åŒ
action = actor_fn(observations=observation)

# Action chunking å¤„ç†ï¼ˆevaluation.py å·²é€‚é…ï¼‰
if len(action_queue) == 0:
    action = np.array(action).reshape(-1, action_dim)
    action_chunk_len = action.shape[0]
    for a in action:
        action_queue.append(a)
else:
    have_new_action = False

action = action_queue.pop(0)
```

---

## è¿è¡Œç¤ºä¾‹å¯¹æ¯”

### JAX ç‰ˆæœ¬

```bash
python main.py \
    --env_name=halfcheetah-medium-v2 \
    --offline_steps=1000000 \
    --online_steps=0 \
    --agent=agents/acfql_bc_distill.py \
    --horizon_length=5
```

### PyTorch ç‰ˆæœ¬

```bash
python train_bc.py \
    --env_name=halfcheetah-medium-v2 \
    --offline_steps=1000000 \
    --horizon_length=5 \
    --action_chunking=False
```

**ä¸»è¦åŒºåˆ«ï¼š**
- PyTorch ç‰ˆæœ¬ä¸éœ€è¦ `--agent` å‚æ•°ï¼ˆagent å›ºå®šä¸º BCï¼‰
- PyTorch ç‰ˆæœ¬ä¸éœ€è¦ `--online_steps`ï¼ˆçº¯ BCï¼‰
- PyTorch ç‰ˆæœ¬ç”¨ `--action_chunking` æ§åˆ¶æ˜¯å¦ä½¿ç”¨åºåˆ—é‡‡æ ·

---

## è¾“å‡ºæ–‡ä»¶å¯¹æ¯”

### ä¿å­˜çš„æ–‡ä»¶

| JAX ç‰ˆæœ¬ | PyTorch ç‰ˆæœ¬ |
|---------|-------------|
| `flags.json` | `flags.json` âœ… |
| `offline_agent.csv` | `offline_agent.csv` âœ… |
| `eval.csv` | `eval.csv` âœ… |
| `checkpoint_{step}/` (ç›®å½•) | `checkpoint_{step}.pt` (æ–‡ä»¶) |
| `final/` (ç›®å½•) | `final_model.pt` (æ–‡ä»¶) |
| `token.tk` | `token.tk` âœ… |

### æ£€æŸ¥ç‚¹æ ¼å¼

```python
# JAX ç‰ˆæœ¬
checkpoint/
  â”œâ”€â”€ agent/
  â”‚   â”œâ”€â”€ network/
  â”‚   â””â”€â”€ ...
  â””â”€â”€ metadata.json

# PyTorch ç‰ˆæœ¬
checkpoint.pt  # å•ä¸ªæ–‡ä»¶ï¼ŒåŒ…å«æ‰€æœ‰å†…å®¹
{
    'actor': actor.state_dict(),
    'critic': critic.state_dict(),
    'target_critic': target_critic.state_dict(),
    'actor_optimizer': actor_optimizer.state_dict(),
    'critic_optimizer': critic_optimizer.state_dict(),
    'config': config,
    'step': step,
}
```

---

## å…³é”®å·®å¼‚æ€»ç»“

### 1. éšæœºæ•°ç®¡ç†

| JAX | PyTorch |
|-----|---------|
| æ˜¾å¼ RNG ä¼ é€’ | å…¨å±€éšæœºæ•°çŠ¶æ€ |
| `jax.random.split()` | `torch.manual_seed()` |
| å‡½æ•°å¼ç¼–ç¨‹ | å‘½ä»¤å¼ç¼–ç¨‹ |

### 2. Agent æ›´æ–°

| JAX | PyTorch |
|-----|---------|
| å‡½æ•°å¼ï¼ˆè¿”å›æ–° agentï¼‰ | åŸåœ°æ›´æ–° |
| `agent, info = agent.update()` | `info = agent.update()` |
| ä¸å¯å˜æ•°æ®ç»“æ„ | å¯å˜æ•°æ®ç»“æ„ |

### 3. æ¨¡å‹ä¿å­˜

| JAX | PyTorch |
|-----|---------|
| è‡ªå®šä¹‰ checkpoint æ ¼å¼ | æ ‡å‡† `.pt` æ ¼å¼ |
| éœ€è¦è¾…åŠ©å‡½æ•° | å†…ç½® `save()`/`load()` |
| ç›®å½•ç»“æ„ | å•ä¸ªæ–‡ä»¶ |

### 4. é…ç½®ç®¡ç†

| JAX | PyTorch |
|-----|---------|
| é…ç½®æ–‡ä»¶ (`.py`) | å‘½ä»¤è¡Œå‚æ•° + dataclass |
| `ml_collections.ConfigDict` | `dataclass` |
| `--agent=agents/xxx.py` | ç›´æ¥åœ¨ä»£ç ä¸­æŒ‡å®š |

---

## è¿ç§»æ¸…å•

å¦‚æœä½ æƒ³ä» JAX ç‰ˆæœ¬è¿ç§»åˆ° PyTorch ç‰ˆæœ¬ï¼š

### âœ… å®Œå…¨å…¼å®¹çš„éƒ¨åˆ†
- [x] ç¯å¢ƒåŠ è½½ï¼ˆ`make_env_and_datasets`ï¼‰
- [x] æ•°æ®é›†å¤„ç†ï¼ˆ`process_train_dataset`ï¼‰
- [x] æ•°æ®é‡‡æ ·ï¼ˆ`sample` / `sample_sequence`ï¼‰
- [x] è¯„ä¼°æµç¨‹ï¼ˆ`evaluate`ï¼‰
- [x] æ—¥å¿—è®°å½•ï¼ˆ`LoggingHelper`ï¼‰
- [x] Action chunking æ‰§è¡Œ

### âš ï¸ éœ€è¦é€‚é…çš„éƒ¨åˆ†
- [ ] Agent åˆ›å»ºæ¥å£ä¸åŒ
- [ ] æ›´æ–°æ–¹å¼ä¸åŒï¼ˆå‡½æ•°å¼ vs åŸåœ°ï¼‰
- [ ] éšæœºæ•°ç®¡ç†ä¸åŒ
- [ ] æ¨¡å‹ä¿å­˜æ ¼å¼ä¸åŒ

### ğŸ”„ å¯é€‰çš„æ”¹è¿›
- [ ] æ·»åŠ  `torch.compile()` åŠ é€Ÿ
- [ ] æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒ
- [ ] æ·»åŠ åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
- [ ] æ·»åŠ æ›´å¤šè¶…å‚æ•°åˆ°å‘½ä»¤è¡Œ

---

## æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | JAX | PyTorch |
|------|-----|---------|
| è®­ç»ƒé€Ÿåº¦ | âš¡âš¡âš¡âš¡ (JIT ç¼–è¯‘) | âš¡âš¡âš¡ (ç¨æ…¢) |
| å†…å­˜ä½¿ç”¨ | ğŸ’¾ğŸ’¾ğŸ’¾ | ğŸ’¾ğŸ’¾ğŸ’¾ğŸ’¾ (ç¨å¤š) |
| æ˜“ç”¨æ€§ | â­â­â­ | â­â­â­â­â­ |
| è°ƒè¯•å‹å¥½ | â­â­ | â­â­â­â­â­ |
| ç”Ÿæ€ç³»ç»Ÿ | â­â­â­ | â­â­â­â­â­ |

---

## å¿«é€Ÿå‚è€ƒ

### å¯åŠ¨è®­ç»ƒ

```bash
# åŸºæœ¬è®­ç»ƒ
python train_bc.py --env_name=halfcheetah-medium-v2

# Action chunking
python train_bc.py --env_name=halfcheetah-medium-v2 --action_chunking=True

# Robomimic
python train_bc.py --env_name=lift-mh-low_dim --action_chunking=True --horizon_length=10
```

### åŠ è½½æ¨¡å‹è¯„ä¼°

```python
from agents.bc_agent import BCAgent, BCAgentConfig

config = BCAgentConfig(...)
agent = BCAgent.create(observation_shape, action_dim, config)
agent.load('runs/fbc/.../final_model.pt')

# è¯„ä¼°
from evaluation import evaluate
stats, _, _ = evaluate(agent, env, action_dim, num_eval_episodes=100)
```

---

å¦‚æœ‰é—®é¢˜ï¼Œè¯·å¯¹æ¯”ï¼š
- JAX ç‰ˆæœ¬: `main.py`
- PyTorch ç‰ˆæœ¬: `train_bc.py`
